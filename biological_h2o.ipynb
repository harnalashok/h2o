{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "h2o",
      "language": "python",
      "name": "h2o"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "biological_h2o.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harnalashok/h2o/blob/master/biological_h2o.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbRyuybGbHBf"
      },
      "source": [
        "\"\"\"\n",
        "Last amended: 1st Jan, 2021\n",
        "My folder: C:\\Users\\Administrator\\OneDrive\\Documents\\biological_response\n",
        "Data Source: https://www.kaggle.com/c/bioresponse/overview\n",
        "\n",
        "Objectives:\n",
        "        i)  Experiments in neural network and Deeplearning\n",
        "        ii) Understanding wt-initialization strategy\n",
        "       iii) Learning to work in h2o\n",
        "        iv)  h2o on Google colab\n",
        "         v) Drug Designing\n",
        "\n",
        "\n",
        "DO NOT EXECUTE THIS CODE IN SPYDER--IT MAY FAIL\n",
        "\n",
        "Ref:\n",
        "Machine Learning with python and H2O\n",
        "   https://www.h2o.ai/wp-content/uploads/2018/01/Python-BOOKLET.pdf\n",
        "H2o deeplearning (latest) booklet\n",
        "   http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/DeepLearningBooklet.pdf\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srPrKzt3dej6"
      },
      "source": [
        "# -3.0 Install java run-time\r\n",
        "! apt-get install default-jre\r\n",
        "!java -version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC92y6gT8um6"
      },
      "source": [
        "# https://medium.com/@naeemasvat.na/how-to-use-h2o-in-google-colab-b69ba539ab1a\r\n",
        "# -2.0 Install h2o\r\n",
        "! pip install h2o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9zoKM0Af5uc"
      },
      "source": [
        "# -1.0 Mount your google drive \n",
        "#      so that you can access data files \n",
        "#      on your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4546jBmQglJ5"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsv7-luZbHBj"
      },
      "source": [
        "# 1.0 Call libraries\n",
        "%reset -f\n",
        "import pandas as pd\n",
        "import h2o\n",
        "import os\n",
        "# 1.1\n",
        "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do-4MK4tbHBj"
      },
      "source": [
        "# 1.2 Display output of multiple commands from a cell\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_9b6t40bHBk"
      },
      "source": [
        "# 2. Start h2o\n",
        "h2o.init(max_mem_size = \"2G\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkMyI-EHbHBl"
      },
      "source": [
        "# 3. Change working folder and read bio_response data\n",
        "# os.chdir(\"C:\\\\Users\\\\Administrator\\\\OneDrive\\\\Documents\\\\biological_response\")\n",
        "# os.chdir(\"D:\\\\data\\\\OneDrive\\\\Documents\\\\biological_response\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdVyHafKbHBm"
      },
      "source": [
        "# 3.1 Read data file (colab code)\n",
        "bio =h2o.import_file(\"/content/drive/MyDrive/MiscFiles/bio_response.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFPA8zWXbHBn"
      },
      "source": [
        "# 3.2 Explore\n",
        "type(bio)           #  h2o.frame.H2OFrame\n",
        "\n",
        "# 3.3\n",
        "bio.shape\n",
        "bio.head(3)     # bio.head().as_data_frame()\n",
        "bio.tail(3)     # bio.tail().as_data_frame()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zpi1IbDXbHBn"
      },
      "source": [
        "# 3.4 Transform target to factor column\n",
        "bio['Activity'] = bio['Activity'].asfactor()\n",
        "\n",
        "# 3.4 How many factor levels this columns has\n",
        "bio['Activity'].levels()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PR4d_H6bHBo"
      },
      "source": [
        "# 3.5 Which are predictors and which one is target column\n",
        "col = bio.columns\n",
        "x = col[1:]\n",
        "y = \"Activity\"\n",
        "x[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg7Ctc2GbHBp"
      },
      "source": [
        "# 4.0 Split the dataset into train/test\n",
        "\n",
        "train,test = bio.split_frame(ratios= [0.7])\n",
        "train.shape\n",
        "test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdrxkX8krUKd"
      },
      "source": [
        "# Weight Initialization strategy\r\n",
        "Ref: [StackOveflow answer this](https://stats.stackexchange.com/a/47604/78454) and [this.](https://stats.stackexchange.com/questions/204114/deep-neural-network-weight-initialization)<br>\r\n",
        "Ref: Write formulas using latex notations. [See this reference](http://www.malinc.se/math/latex/basiccodeen.php)<br>\r\n",
        "Let us assume you are using sigmoid neuron ie logistic neurons.<br>\r\n",
        "\r\n",
        "The logistic function is close to flat for large positive or negative inputs. The derivative at an input of 2\r\n",
        "is about 1/10, but at 10 the derivative is about 1/22000. This means that if the input of a logistic neuron is 10 then, for a given training signal, the neuron will learn about 2200 times slower that if the input was 2.<br>\r\n",
        "\r\n",
        "If you want the neuron to learn quickly, you either need to produce a huge training signal (such as with a cross-entropy loss function) or you want the derivative to be large. To make the derivative large, you set the initial weights so that you often get inputs in the range [−4,4].<br>\r\n",
        "\r\n",
        "The initial weights you give might or might not work. It depends on how the inputs are normalized. If the inputs are normalized to have mean 0 and standard deviation 1, then a random sum of d terms with weights uniform on\r\n",
        "$ [1/\\sqrt{d}, -1/\\sqrt{d} ]$  will have mean 0 and variance 1/3, independent of d. The probability that you get a sum outside of [−4,4] is small. That means as you increase d, you are not causing the neurons to start out saturated so that they don't learn.<br>\r\n",
        "\r\n",
        "With inputs which are not normalized, those weights may not be effective at avoiding saturation.<br><br>\r\n",
        "Some Maths:<br>\r\n",
        "Assume there are d inputs and all are normalized. So input-signal mean = 0 and input signal var = 1. If weight from input to next neuron is 'w' and it is uniformly distributed as: $ [1/\\sqrt{d}, -1/\\sqrt{d} ]$, then <br><br>\r\n",
        "E($ \\sum_ {i=1}^{d} w $ ) =  d * E($ \\sum_ {}^{} w $ ) = d * 0 = 0. And variance of sum of inputs at a neuron is:  <br>\r\n",
        "Var($ \\sum_ {i=1}^{d} w $ ) = var(w1) + var(w2)+ var(w3) + ...d-terms <br><br>\r\n",
        "For a uniform distribution of [a,b], varaince is: \r\n",
        "$ (b-a)^2 /12 $ <br>\r\n",
        "Therefore, for uniform distribution $ [1/\\sqrt{d}, -1/\\sqrt{d} ]$ , variance is 1/(3d).<br>\r\n",
        "As all w's are identically distributed, therefore, <br>\r\n",
        "Var(w1) + var(w2) +...d-terms is = 1/3.<br><br>\r\n",
        "*Glorot initialization for sigmoid activation*<br>\r\n",
        "$fan_{avg} = (fan_{in} + fan_{out})/2 $ <br>\r\n",
        "Normal distribution with mean 0 and variance: $ 1/fan_{avg} $<br>\r\n",
        "OR, a uniform distribution between -r and +r with r= \r\n",
        "$ 1/\\sqrt({3/fan_{avg}}) $ <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY0-9646bHBp"
      },
      "source": [
        "# 4.1 Instantiate a simple deeplearning model\n",
        "#     We vary Initialization wts to see their\n",
        "#     effect on validation error\n",
        "#     Ref: Weights initialization depend on the activation function being used. \n",
        "\n",
        "df = []\n",
        "initial_wt_dist = [\"Normal\", \"Uniform Adaptive\", \"Uniform\" ]\n",
        "\n",
        "# 4.2\n",
        "for i in range(3):\n",
        "    # 4.3 Instantiate the model\n",
        "    dl =H2ODeepLearningEstimator(\n",
        "                                   distribution=\"bernoulli\",\n",
        "                                   activation = \"Tanh\",\n",
        "                                   hidden = [64,32,16],\n",
        "                                   epochs = 100,           # Even though epochs are 100,\n",
        "                                                           # iterations stop very early. \n",
        "                                                           # Progress bar after some time to \n",
        "                                                           # suddenly jumps to 100\n",
        "                                   score_each_iteration = True,\n",
        "                                   initial_weight_distribution = initial_wt_dist[i]\n",
        "                                  )\n",
        "    # 4.4 Begin training\n",
        "    dl.train(\n",
        "              x= x,            # Predictor columns\n",
        "              y= y,            # Target\n",
        "              training_frame=train,  # training data\n",
        "              validation_frame = test\n",
        "             )\n",
        "\n",
        "    # 4.5 Append dl object to list\n",
        "    df.append(dl)       \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31wmgcQUbHBq"
      },
      "source": [
        "# 4.6 Get scoring history for each of the three models\n",
        "df_normal = df[0].scoring_history()\n",
        "df_ua = df[1].scoring_history()\n",
        "df_un = df[2].scoring_history()\n",
        "# 4.7\n",
        "df_normal.columns\n",
        "#4.8\n",
        "df_normal.head(4)\n",
        "df_normal.tail(4)\n",
        "df_ua[['validation_classification_error','training_classification_error']].head(3)\n",
        "df_un[['validation_classification_error','training_classification_error']].head(3)\n",
        "df_normal[['validation_classification_error','training_classification_error']].head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhWOReI0bHBq"
      },
      "source": [
        "# 5.0 Plot validation errors for difft\n",
        "#     initialization schemes:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# 5.1\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "# 5.2\n",
        "_=ax.plot(df_ua[['iterations']],df_ua[['validation_classification_error']],label = \"Uniform Adaption\", color = \"red\")\n",
        "_=ax.plot(df_un[['iterations']],df_un[['validation_classification_error']],label = \"Uniform\", color = \"black\")\n",
        "_=ax.plot(df_normal[['iterations']],df_normal[['validation_classification_error']], label = \"Normal\",   color = \"blue\")\n",
        "_=ax.legend()\n",
        "_= ax.set_title(\"Effect of wt-initialization strategy\")\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_B8yJldbHBr"
      },
      "source": [
        "# 6.0 \n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "_=ax.plot(df_ua[['training_classification_error']]    , label = \"Uniform Adaption\", color = \"red\")\n",
        "_=ax.plot(df_un[['training_classification_error']]    , label = \"Uniform\",          color = \"black\")\n",
        "_=ax.plot(df_normal[['training_classification_error']], label = \"Normal\",           color = \"blue\")\n",
        "_=ax.legend()\n",
        "ax.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxc-gUeibHBr"
      },
      "source": [
        "# 7.0\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "_=ax.plot(df_ua[['iterations']], df_ua[['training_classification_error']]    , label = \"Training error\",    color = \"red\")\n",
        "_=ax.plot(df_ua[['iterations']], df_ua[['validation_classification_error']]  , label = \"Validation error\",color = \"black\")\n",
        "_=ax.legend()\n",
        "_=ax.set_title(\"Learning Curve--Overfitting is obvious\")\n",
        "ax.grid(which='major', linestyle='-', linewidth='0.5', color='red')\n",
        "# Turn on the minor TICKS, which are required for the minor GRID\n",
        "ax.minorticks_on()\n",
        "ax.grid(which='minor', linestyle=':', linewidth='0.5', color='black')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfJih7hsbHBs"
      },
      "source": [
        "# 8.0 \n",
        "dl =H2ODeepLearningEstimator(\n",
        "                               distribution=\"bernoulli\",\n",
        "                               activation = \"rectifierwithdropout\",   # CHANGED\n",
        "                               hidden = [32,32,32],\n",
        "                               epochs = 25,                      # CHANGED\n",
        "                                                                # Iterations stop very early. You can this from \n",
        "                                                                # progress bar that suddenly jumps to 100% after a time\n",
        "                               hidden_dropout_ratios = [0.4,0.5,0.5] ,  # ADDED\n",
        "                               #input_dropout_ratio = 0.2,\n",
        "                               l1= 1e-5,   \n",
        "                               l2= 1e-5,\n",
        "                               score_each_iteration = True,\n",
        "                               initial_weight_distribution = \"Uniform Adaptive\",\n",
        "                               variable_importances = True\n",
        "                              )\n",
        "\n",
        "dl.train(\n",
        "          x= x,            # Predictor columns\n",
        "          y= y,            # Target\n",
        "          training_frame=train,  # training data\n",
        "          validation_frame = test\n",
        "         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX8VLMr_vUsd"
      },
      "source": [
        "# 9.0\r\n",
        "dl.predict(test)\r\n",
        "dl.logloss()\r\n",
        "dl.auc()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZPgq70HbHBs"
      },
      "source": [
        "# 9.1\r\n",
        "dx1 = dl.scoring_history()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VG1V8x1XbHBs"
      },
      "source": [
        "# 9.2\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "_=ax.plot(dx1[['iterations']],dx1[['training_classification_error']]    , label = \"Training error\",    color = \"red\")\n",
        "_=ax.plot(dx1[['iterations']],dx1[['validation_classification_error']]  , label = \"Validation error\",color = \"black\")\n",
        "_=ax.legend()\n",
        "_= ax.set_title(\"Learning Curve--No overfitting\")\n",
        "_= ax.set_ylim([0.10,0.40])\n",
        "ax.minorticks_on()\n",
        "ax.grid(which = \"major\", color = \"red\")\n",
        "ax.grid(which = \"minor\", linestyle = \"--\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqj2TFt8xj6A"
      },
      "source": [
        "# 9.3\r\n",
        "fig = plt.figure()\r\n",
        "ax = fig.add_subplot(111)\r\n",
        "\r\n",
        "_=ax.plot(dx[['iterations']],dx[['training_classification_error']]    , label = \"Training error\",    color = \"red\")\r\n",
        "_=ax.plot(dx[['iterations']],dx[['validation_classification_error']]  , label = \"Validation error\",color = \"black\")\r\n",
        "_=ax.legend()\r\n",
        "_= ax.set_title(\"Learning Curve--No overfitting\")\r\n",
        "_= ax.set_ylim([0.10,0.40])\r\n",
        "ax.minorticks_on()\r\n",
        "ax.grid(which = \"major\", color = \"red\")\r\n",
        "ax.grid(which = \"minor\", linestyle = \"--\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZg5NT9TbHBs"
      },
      "source": [
        "# 10.0\n",
        "# https://stackoverflow.com/q/45442608/3282777\n",
        "# Feature importance is in decreasing order\n",
        "#  Variable Importance considers the weights connecting\n",
        "#  the input features to the first two hidden layers.\n",
        "#   The higher the connecting weights, more impt the feature is\n",
        "\n",
        "dl.varimp()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re4N7c54bHBt"
      },
      "source": [
        "import numpy as np\n",
        "# https://stackoverflow.com/q/45442608/3282777\n",
        "f_impt = pd.DataFrame.from_records(dl.varimp(), columns = [\"feature\", \"relative_importance\", \"scaled_importance\", \"percentage\"])\n",
        "f_impt\n",
        "f_impt['scaled_importance']/np.sum(f_impt['scaled_importance'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00c1KS0ubHBt"
      },
      "source": [
        "help(dl.varimp())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unwGqImvuvvK"
      },
      "source": [
        "##################### Examining Regularization ##########################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGgtthEj4hJZ"
      },
      "source": [
        "# 11.0\r\n",
        "train,test = bio.split_frame(ratios= [0.7])\r\n",
        "train.shape\r\n",
        "test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JspZ_mi6mS6v"
      },
      "source": [
        "# 11.1 No regularization\r\n",
        "dl =H2ODeepLearningEstimator(\r\n",
        "                               distribution=\"bernoulli\",\r\n",
        "                               activation = \"rectifier\",   # CHANGED\r\n",
        "                               hidden = [100,64,32],\r\n",
        "                               epochs = 500,                      # CHANGED\r\n",
        "                                                                # Iterations stop very early. You can this from \r\n",
        "                                                                # progress bar that suddenly jumps to 100% after a time\r\n",
        "                               #hidden_dropout_ratios = [0.5,0.5,0.5] ,  # ADDED\r\n",
        "                               #l1= 1e-5,   \r\n",
        "                               #l2= 1e-5,\r\n",
        "                               score_each_iteration = True,\r\n",
        "                               initial_weight_distribution = \"Uniform Adaptive\"\r\n",
        "                               #variable_importances = True\r\n",
        "                              )\r\n",
        "\r\n",
        "dl.train(\r\n",
        "          x= x,            # Predictor columns\r\n",
        "          y= y,            # Target\r\n",
        "          training_frame=train,  # training data\r\n",
        "          validation_frame = test\r\n",
        "         )\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "reg_no = dl.scoring_history()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GyYCXcAj_qJ"
      },
      "source": [
        "#11.2 Dropouts only\r\n",
        "dl =H2ODeepLearningEstimator(\r\n",
        "                               distribution=\"bernoulli\",\r\n",
        "                               activation = \"rectifierwithdropout\",   # CHANGED\r\n",
        "                               hidden = [100,64,32],\r\n",
        "                               epochs = 500,                      # CHANGED\r\n",
        "                                                                # Iterations stop very early. You can this from \r\n",
        "                                                                # progress bar that suddenly jumps to 100% after a time\r\n",
        "                               hidden_dropout_ratios = [0.5,0.5,0.5] ,  # ADDED\r\n",
        "                               #l1= 1e-5,   \r\n",
        "                               #l2= 1e-5,\r\n",
        "                               score_each_iteration = True,\r\n",
        "                               initial_weight_distribution = \"Uniform Adaptive\"\r\n",
        "                               #variable_importances = True\r\n",
        "                              )\r\n",
        "\r\n",
        "dl.train(\r\n",
        "          x= x,            # Predictor columns\r\n",
        "          y= y,            # Target\r\n",
        "          training_frame=train,  # training data\r\n",
        "          validation_frame = test\r\n",
        "         )\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "reg_drop = dl.scoring_history()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFi4X_TmptSQ"
      },
      "source": [
        "# 11.3  l1/l2\r\n",
        "dl =H2ODeepLearningEstimator(\r\n",
        "                               distribution=\"bernoulli\",\r\n",
        "                               activation = \"rectifier\",   # CHANGED\r\n",
        "                               hidden = [100,64,32],\r\n",
        "                               epochs = 500,                      # CHANGED\r\n",
        "                                                                # Iterations stop very early. You can this from \r\n",
        "                                                                # progress bar that suddenly jumps to 100% after a time\r\n",
        "                               #hidden_dropout_ratios = [0.5,0.5,0.5] ,  # ADDED\r\n",
        "                               l1= 1e-5,   \r\n",
        "                               l2= 1e-5,\r\n",
        "                               score_each_iteration = True,\r\n",
        "                               initial_weight_distribution = \"Uniform Adaptive\"\r\n",
        "                               #variable_importances = True\r\n",
        "                              )\r\n",
        "\r\n",
        "dl.train(\r\n",
        "          x= x,            # Predictor columns\r\n",
        "          y= y,            # Target\r\n",
        "          training_frame=train,  # training data\r\n",
        "          validation_frame = test\r\n",
        "         )\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "reg_l1 = dl.scoring_history()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAnJP03LsVcN"
      },
      "source": [
        "# 11.4 \r\n",
        "# dropouts + l1 + l2\r\n",
        "dl =H2ODeepLearningEstimator(\r\n",
        "                               distribution=\"bernoulli\",\r\n",
        "                               activation = \"rectifierwithdropout\",   # CHANGED\r\n",
        "                               hidden = [100,64,32],\r\n",
        "                               epochs = 500,                      # CHANGED\r\n",
        "                                                                # Iterations stop very early. You can this from \r\n",
        "                                                                # progress bar that suddenly jumps to 100% after a time\r\n",
        "                               hidden_dropout_ratios = [0.5,0.5,0.5] ,  # ADDED\r\n",
        "                               l1= 1e-5,   \r\n",
        "                               l2= 1e-5,\r\n",
        "                               score_each_iteration = True,\r\n",
        "                               initial_weight_distribution = \"Uniform Adaptive\"\r\n",
        "                               #variable_importances = True\r\n",
        "                              )\r\n",
        "\r\n",
        "dl.train(\r\n",
        "          x= x,            # Predictor columns\r\n",
        "          y= y,            # Target\r\n",
        "          training_frame=train,  # training data\r\n",
        "          validation_frame = test\r\n",
        "         )\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "reg_all = dl.scoring_history()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvKT_yA7zJ43"
      },
      "source": [
        "# 11.5 dropouts + l1 + l2 + input + stronger l1/l2\r\n",
        "dl =H2ODeepLearningEstimator(\r\n",
        "                               distribution=\"bernoulli\",\r\n",
        "                               activation = \"rectifierwithdropout\",   # CHANGED\r\n",
        "                               hidden = [100,64,32],\r\n",
        "                               epochs = 500,                      # CHANGED\r\n",
        "                                                                # Iterations stop very early. You can this from \r\n",
        "                                                                # progress bar that suddenly jumps to 100% after a time\r\n",
        "                               hidden_dropout_ratios = [0.5,0.5,0.5] ,  # ADDED\r\n",
        "                               l1= 1e-4,   \r\n",
        "                               l2= 1e-4,\r\n",
        "                               input_dropout_ratio = 0.2,     # Added\r\n",
        "                               mini_batch_size = 10,          # Added\r\n",
        "                               stopping_rounds= 20,           # Added \r\n",
        "                               score_each_iteration = True,\r\n",
        "                               initial_weight_distribution = \"Uniform Adaptive\"\r\n",
        "                               #variable_importances = True\r\n",
        "                              )\r\n",
        "\r\n",
        "dl.train(\r\n",
        "          x= x,            # Predictor columns\r\n",
        "          y= y,            # Target\r\n",
        "          training_frame=train,  # training data\r\n",
        "          validation_frame = test\r\n",
        "         )\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "reg_all_in = dl.scoring_history()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aXU0hDak162"
      },
      "source": [
        "# 12.0 Draw all the four now\r\n",
        "fig = plt.figure(figsize=(20,5))\r\n",
        "\r\n",
        "ax = fig.add_subplot(151)\r\n",
        "\r\n",
        "sc = reg_no\r\n",
        "\r\n",
        "_=ax.plot(sc[['iterations']],sc[['training_classification_error']]    , label = \"Training error\",    color = \"red\")\r\n",
        "_=ax.plot(sc[['iterations']],sc[['validation_classification_error']]  , label = \"Validation error\",color = \"black\")\r\n",
        "_=ax.legend()\r\n",
        "_= ax.set_title(\"No regularization\")\r\n",
        "_= ax.set_ylim([0.10,0.40])\r\n",
        "ax.minorticks_on()\r\n",
        "ax.grid(which = \"major\", color = \"red\")\r\n",
        "ax.grid(which = \"minor\", linestyle = \"--\")\r\n",
        "\r\n",
        "#######################33\r\n",
        "\r\n",
        "sc = reg_drop\r\n",
        "\r\n",
        "ax = fig.add_subplot(152)\r\n",
        "\r\n",
        "_=ax.plot(sc[['iterations']],sc[['training_classification_error']]    , label = \"Training error\",    color = \"red\")\r\n",
        "_=ax.plot(sc[['iterations']],sc[['validation_classification_error']]  , label = \"Validation error\",color = \"black\")\r\n",
        "_=ax.legend()\r\n",
        "_= ax.set_title(\"Only dropouts\")\r\n",
        "_= ax.set_ylim([0.10,0.40])\r\n",
        "ax.minorticks_on()\r\n",
        "ax.grid(which = \"major\", color = \"red\")\r\n",
        "ax.grid(which = \"minor\", linestyle = \"--\")\r\n",
        "\r\n",
        "###############3\r\n",
        "\r\n",
        "\r\n",
        "ax = fig.add_subplot(153)\r\n",
        "sc = reg_l1\r\n",
        "_=ax.plot(sc[['iterations']],sc[['training_classification_error']]    , label = \"Training error\",    color = \"red\")\r\n",
        "_=ax.plot(sc[['iterations']],sc[['validation_classification_error']]  , label = \"Validation error\",color = \"black\")\r\n",
        "_=ax.legend()\r\n",
        "_= ax.set_title(\"Only l1/l2\")\r\n",
        "_= ax.set_ylim([0.10,0.40])\r\n",
        "ax.minorticks_on()\r\n",
        "ax.grid(which = \"major\", color = \"red\")\r\n",
        "ax.grid(which = \"minor\", linestyle = \"--\")\r\n",
        "\r\n",
        "#######################\r\n",
        "\r\n",
        "ax = fig.add_subplot(154)\r\n",
        "sc = reg_all\r\n",
        "_=ax.plot(sc[['iterations']],sc[['training_classification_error']]    , label = \"Training error\",    color = \"red\")\r\n",
        "_=ax.plot(sc[['iterations']],sc[['validation_classification_error']]  , label = \"Validation error\",color = \"black\")\r\n",
        "_=ax.legend()\r\n",
        "_= ax.set_title(\"Droputs + l1 + l2\")\r\n",
        "_= ax.set_ylim([0.10,0.40])\r\n",
        "ax.minorticks_on()\r\n",
        "ax.grid(which = \"major\", color = \"red\")\r\n",
        "ax.grid(which = \"minor\", linestyle = \"--\")\r\n",
        "\r\n",
        "###########################\r\n",
        "\r\n",
        "ax = fig.add_subplot(155)\r\n",
        "sc = reg_all_in\r\n",
        "_=ax.plot(sc[['iterations']],sc[['training_classification_error']]    , label = \"Training error\",    color = \"red\")\r\n",
        "_=ax.plot(sc[['iterations']],sc[['validation_classification_error']]  , label = \"Validation error\",color = \"black\")\r\n",
        "_=ax.legend()\r\n",
        "_= ax.set_title(\"Droputs + l1 + l2+ in\")\r\n",
        "_= ax.set_ylim([0.10,0.40])\r\n",
        "ax.minorticks_on()\r\n",
        "ax.grid(which = \"major\", color = \"red\")\r\n",
        "ax.grid(which = \"minor\", linestyle = \"--\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aipTvYVlbHBu"
      },
      "source": [
        "#################"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}